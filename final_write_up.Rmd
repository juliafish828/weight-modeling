---
title: "ST563 Project"
author: "Julia Fish"
date: "2025-03-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

## Introduction

Obesity levels have sky rocketed all around the world in the last few decades. Many aspects of daily life have changed since this started to become more widespread. These include a technological boom, increases in recreational drug use, food processing techniques, and a variety of others. Although life is undeniably different than it has been in the past, there are also scientific and genetic factors that have been linked to an increase risk/presence of obesity. This leaves the obvious questions: what details of one's life matter most for being able to predict weight level? And do the important factors change depending on the weight level in question? One attempt to move closer to answering those questions is to analyze these factors and their perceived effect on weight level, which will be done by determining the best performing model on obesity level with a relevant data set throughout the report below.

The data set that will be used for this exploration was collected by Fabio Mendoza Palechor and Alexis de la Hoz Manotas from la Universidad de la Costa in Colombia. This data collection surveyed individuals from 3 Hispanic countries: Mexico, Peru, and Colombia. Multiple different aspects of their lives were asked about, including family history, diet, and physical activity, among others (Pachelor Manotas 2019). Analysis of this data will hopefully allow us to achieve the goal of this report, which is to model the relationship between obesity level and some predictors deemed most important.

It is stated in the abstract from the introductory paper that 77% of this data is synthetic, with the rest being personal accounts (Pachelor Manotas 2019). For this reason, there can be no actual takeaways of potential patterns that can be applied to real world scenarios. With this in mind, pursuit of the best model for weight level using this data continues.

Below, the data set will be read in, and the potential predictors will be discussed.

# Reading in the Data and Variable Definition

```{r packages, results='hide', message = FALSE}
library(dplyr)
library(caret)
library(mgcv)
library(gam)
library(corrplot)
library(knitr)
library(glmnet)
library(collinear)
library(psych)
library(car)
library(rpart)
library(kknn)
library(randomForest)
library(fastDummies)
library(ModelMetrics)
library(sparseSVM)
library(e1071)
library(forecast)
library(rpart)
library(tree)
library(rpart.plot)
```

```{r read in data}
obes <- read.csv("ObesityDataSet_raw_and_data_sinthetic.csv", header = TRUE)
head(obes)
```

This data set has 2111 observations and 17 variables. Since the variable names are not quite clear, they are more colloquially defined (in order) below:

1) Gender
2) Age
3) Height
4) Weight
5) Do you have a family history of higher weight?
6) Do you eat high calorie food regularly?
7) Are there vegetables in your meals regularly?
8) How many meals per day usually?
9) Do you eat between meals?
10) Do you smoke?
11) How much water do you drink daily (in levels/categories)?
12) Do you monitor caloric intake?
13) How many days do you exercise a week (in levels/categories)?
14) How many hours a day are spent using technology (in levels/categories)?
15) How often do you drink alcohol (in levels/categories)?
16) What type of transportation do you use most?
17) What is your weight level?

# Initial Variable Selection

Before analysis begins, we want to individually explore each variable. Before doing this, let us first eliminate height and weight from analysis. The respinse variable (weight class) is defined as a BMI range, ad BMI is defined solely on height and weight values. If given both of these measurements, no other variables would need to be known in order to predict weight class with 100% accuracy. In trying to explore nontrivial factors that could impact weight, we will remove these before continuing any further.

```{r remove height and weight}
obes <- obes %>% select(-Height, -Weight)
```

# Data/Variable Exploration

Next, we will explore the data set by making sure each variable is of the type that contextually makes sense, as well as identifying if there are missing values anywhere in the data set.

```{r explore}
type_missing <- matrix(data=c(sapply(obes, class), sapply(obes,anyNA)), ncol = 15, nrow = 2,
                       byrow =TRUE,dimnames=list(c("type","missing")))
colnames(type_missing) <- colnames(obes)
type_missing
```
Looking at the matrix above, let's start with the second row: All of these values are false, meaning that there are no missing values in any variable of this data set.

Having seen no issues with the second row of information, we will move to looking at the first row. Comparing the variable types to the list of informal variable names (for context), most variables need to be shifted. Namely, all variables past column 3 (family history with heavy weight) should be factors, as well as Gender. We will also make family history of overweight a 0/1 numeric response. We have a lack of non-factor variables, and the binary response makes the effect of having a family history with heavy weight feasibly represented as a numeric predictor.

Before we change any variable representation, we will investigate the 5 numeric variables that should theoretically be factor variables to ensure that they are defined as stated in the paper. This is done below:

```{r between values}
num_to_fac <- select(obes, c("FCVC", "NCP", "CH2O", "FAF", "TUE"))
cols2 <- colnames(num_to_fac)
par(mfrow=c(2,3))

for (i in num_to_fac){
  plot(i)
}

```

As the plots indicate, there are values between the 3-4 possible levels stated in the paper. This means that there are many values between what should be the only possible responses. This is most likely due to the simulated responses discussed earlier in this report. We will round each response to its nearest integer value in order to restore the integrity of the variable's definition. After this, we will plot the data again to ensure that each value is correct.

```{r restore}
for (i in 1:nrow(num_to_fac)) {
   for (j in 1:ncol(num_to_fac)) {
        if (num_to_fac[i,j] >= 0 & num_to_fac[i,j] < 0.5){
           num_to_fac[i,j] <- 0
        } else if (num_to_fac[i,j] >= 0.5 & num_to_fac[i,j] < 1.5){
           num_to_fac[i,j] <- 1
        } else if (num_to_fac[i,j] >= 1.5 & num_to_fac[i,j] < 2.5){
           num_to_fac[i,j] <- 2
        } else if (num_to_fac[i,j] >= 2.5 & num_to_fac[i,j] < 3.5){
           num_to_fac[i,j] <- 3
        } else if (num_to_fac[i,j] >= 3.5 & num_to_fac[i,j] <= 4){
           num_to_fac[i,j] <- 4
        } # end if statements
    } # end inside for loop
} # end outside for loop

par(mfrow=c(2,3))
for (i in num_to_fac){
  plot(i)
}

par(mfrow=c(1,1))
```

Now that the values accurately represent the variables' definitions, we will place these versions of the variables back into the original data set. Here, we will also express the family history with overweight as a 0/1 numeric response (as explained above).

```{r replace}
obes$FCVC <- num_to_fac$FCVC
obes$NCP <- num_to_fac$NCP
obes$CH2O <- num_to_fac$CH2O
obes$FAF <- num_to_fac$FAF
obes$TUE <- num_to_fac$TUE

obes <- obes %>%
      mutate(family_history_with_overweight = ifelse(family_history_with_overweight == "no",0,1))
```

All of the variables are of the appropriate values; we will now make the changes to the variable type for all that needed to be changed. The "type/missing" matrix will be reproduced to ensure all changes were made without issue.

```{r change}
obes$Gender <- as.factor(obes$Gender)
obes$FAVC <- as.factor(obes$FAVC)
obes$FCVC <- as.factor(obes$FCVC)
obes$NCP <- as.factor(obes$NCP)
obes$CAEC <- as.factor(obes$CAEC)
obes$SMOKE <- as.factor(obes$SMOKE)
obes$CH2O <- as.factor(obes$CH2O)
obes$SCC <- as.factor(obes$SCC)
obes$FAF <- as.factor(obes$FAF)
obes$TUE <- as.factor(obes$TUE)
obes$CALC <- as.factor(obes$CALC)
obes$MTRANS <- as.factor(obes$MTRANS)
obes$NObeyesdad <- as.factor(obes$NObeyesdad)

info2 <- matrix(data=c(sapply(obes, class), sapply(obes,anyNA)), ncol = 15, nrow = 2, byrow = TRUE,
               dimnames=list(c("type","missing")))
colnames(info2) <- colnames(obes)
info2
```

We can take away from this reproduced matrix that each variable is represented correctly and that there are no missing values. Now, let us briefly inspect each variable to look for abnormalities that could impact analysis.

For Age, we will look for unusual values through the 5 number summary.

```{r more}
boxplot(obes$Age, main = "Age")
summary(obes$Age)
```

These numerical summary of Age is mostly what was to be expected. The concentration of lower values contributes to the higher values being outliers through a small IQR. However, there are several of these higher values, all of which logical and valid ages. As a result of this, we will no remove any of these observations.



For the 13 remaining factor variables, we will look at all of the levels for each variable in order to make sure there are no values that are not specified as one of the valid levels for that factor.


```{r levels}
chr <- obes %>% select(-Age, -family_history_with_overweight)
sapply(chr, levels)
```

From the matrix above, we can see that all of the levels for all of the variables is what was expected. None of the variables have values that differ from the levels defined in the paper (i.e. no typos).

Now, we are ready to split our data and begin fitting models to this data set. One final measure before we fit models is to look for multicollinearity within the predictors. This is both to ensure all models are fit appropriately as well as give a baseline for the predictors that should be considered for models that do not do variable selection.

# General Variable Selection

We hope to select a general model that lacks multicollinearity. To work toward this, we will investigate the pairwise correlations for all potential predictors.


```{r corr matrix}
obes_no_resp <- obes %>% select(-NObeyesdad)
cors <- cor_df(obes_no_resp)
head(cors)
```

Since there is high correlation between Age and MTRANS, we will select keeping age for the general model. This relationship is the only one with a high correlation, so all other predictors will remain to be considered.


```{r keep}
obes <- obes %>% select(-MTRANS)
```

# Contextual Variable Selection

When models are run in which variable selection does not occur, we will need a subset of these predictors to use for those models (since all but 2 are factors with <2 levels). Since most other variable selection criteria have been utilized already with 13 predictors still remaining, we will use context to select a few potentially meaningful predictors. Those selected (as well as a reason) are below:

1. Age (healthy weights can change immensely depending on age)
2. Family history of overweight (environment and genetics could play a large role in predicting weight level)
3. Smoke (smoking could decrease appetite/act as stress coping mechanism over overeating, or it could be an indicator of unhealthy habits)
4. CAEC - "Do you eat between meals?" (excessive snacking may prove to be where overconsumption lay in the diets of many)

These predictors will be the four used in models where we cannot rely on variable selection.

# Response Redefinition

Lastly, there are many different weight classes defined in this response. Specifically, there is one for underweight, one for healthy weight, 2 for overweight, ad 3 for obese. In an attempt to limit that number (to hopefully prevent convergence issues later), we will condense down to 4 different weight classes: Underweight, Normal weight, Overweight, and Obese (and ensure that that new variable is a factor as well).

```{r condense}
#Hardcoded due to issues with type taking numeric representation of NObeyesdad
obes$type <- ifelse(obes$NObeyesdad %in% c("Overweight_Level_I",
                                                   "Overweight_Level_II"),
                        "Overweight",
                  ifelse(obes$NObeyesdad %in% c("Obesity_Type_I",
                                                    "Obesity_Type_II",
                                                    "Obesity_Type_III"),
                         "Obese",
                  ifelse(obes$NObeyesdad %in% c("Normal_Weight"),
                         "Normal_Weight",
                  "Insufficient_Weight")))

obes <- obes %>% select(-NObeyesdad)
obes$type <- as.factor(obes$type)
```

We are now ready to split the data into a train/test split and begin fitting models.

## Train and Test Set Split

For this data, we will use an 80-20 split. This split is done below.

```{r split the data}
set.seed(828)

index1 <- createDataPartition(obes$type, p = 0.8,
                             list = FALSE)

obes_train <- obes[index1, ]
obes_test <- obes[-index1, ]
```


Our first model will be a kNN model.

# kNN model

A kNN model is a non-parametric model with a tuning parameter, k. That tuning parameter represents the amount of neighboring points that are used in order to predict the weight levels for unknown points (from cross validation folds). Inference is challenging for this model, since there is no objective model to investigate anf find general takeaways of the predictors' relationships to the response and each other. Unfortunately, kNN does not perform variable selection; it simply keeps all of the predictors that were stated to be considered in the model. We will also standardize the predictors in order to ensure that there are not issues with different scales of the predictor variables.


```{r knn}
#standardizing Age in training set
obes_train_stand <- obes_train
obes_train_stand$Age <- scale(obes_train_stand$Age)

#standardizing Age in test set
obes_test_stand <- obes_test
obes_test_stand$Age <- scale(obes_test_stand$Age)

#making a tuning grid
kgrid <- expand.grid(k=c(1:100))
cv <- trainControl(method = "cv",
                   number = 12)

# kNN with all 4 variables contextually selected above
knn_obes_4 <- train(type ~ Age + family_history_with_overweight +
                        SMOKE + CAEC,
                  data = obes_train_stand,
                  method = "knn",
                  tuneGrid = kgrid,
                  trControl = cv)

k_opt <- as.numeric(knn_obes_4$bestTune)
k_opt
```

The best tuned K value for the model selected above is a value of 10. The final model (using just that value) is fit below to be tested with the test set later in the report:


```{r knn finals}
# kNN with k=10
knn_obes_final <- train(type ~ Age + family_history_with_overweight +
                        SMOKE + CAEC,
                  data = obes_train_stand,
                  method = "knn",
                  tuneGrid = expand.grid(k = k_opt),
                  trControl = trainControl(method = "none"))
```


# Logistic Regression

Next, we will fit a logistic regression model. This is a parametric model because it assumes a generalized linear model form for the relationship between response and predictors. This model does have a tuing parameter, lambda, which is a penalty term for the regression coefficients. Inference is challenging for this model as wel due to its complex fit. As before, this model does not do variable selection, so we will stick with our original 4 variables from the kNN model above. Standardization of the predictors does not need to occur, but dummy variables must be made in place of factor variables. All the created dummy variables are placed into the model.

```{r logistic fit}
obes_train_dummy <- dummy_cols(obes_train, select_columns = c("SMOKE", "CAEC"))
obes_test_dummy <- dummy_cols(obes_test, select_columns = c("SMOKE", "CAEC"))

glm_obes <- cv.glmnet(x = as.matrix(obes_train_dummy |>
                                     dplyr::select(Age,
                                          family_history_with_overweight,
                                          SMOKE_no, SMOKE_yes,
                                          CAEC_Always, CAEC_Frequently,
                                          CAEC_Sometimes, CAEC_no)),
                       y = obes_train_dummy$type,
                       family = "multinomial",
                       alpha = 1)

lambda_value <- glm_obes$lambda.1se
lambda_value
```

The lambda value associated with the smallest cvm value was found, and we will use the lambda one standard error away from that value to fit for the final model (to attempt to steer away from overfitting). Instead of fitting that model directly, we will use the predict function with the above glmnet model and one-standard-error lambda value. Since that outputs the probabilities associated with each class, we will take the largest probability of each class for each observation to have our predicted test set values to use later.

```{r logistic final and prediction}
#defining newx
x_test <- obes_test_dummy %>%
  dplyr::select(Age, family_history_with_overweight, 
                SMOKE_no, SMOKE_yes, 
                CAEC_Always, CAEC_Frequently, 
                CAEC_Sometimes, CAEC_no) %>%
  as.matrix()


glm_prob <- predict(glm_obes, newx = x_test, type = "response", s = lambda_value)
glm_preds <- as.factor(apply(glm_prob, 1, which.max))
```


# Local Logistic Regression

Next, we will fit a GAM model to this data to predict weight level. Though there is a strict additive assumption of the effects of these predictors on obesity level, this model is nonparametric. That is due to the lack of assumptions of any specific form of the relationship between the predictors and the response. Here, we will be tuning on the span of the model, but the degree of the polynomimal splines can also be tuned on as well (here, we only allowed a value of one). This model is not able to be explicitly written out, as it is too complex, which makes inference not possible in a general sense. Lastly, the data do not need to be standardized in order to fit this model appropriately.

This model fit is below:


```{r gamloess 1}
x <- select(obes_train, c("Age", "family_history_with_overweight", "SMOKE", "CAEC"))
y <- obes_train$type

train_control <- trainControl(method = "cv", number = 5)

tune_grid <- expand.grid(span = c(0.1, 0.2, 0.3, 0.4),  
                         degree = c(1))

# Fit the gamLoess model
gamLoess_model <- train(x = x, 
                        y = y, 
                        method = "gamLoess", 
                        trControl = train_control, 
                        tuneGrid = tune_grid)

span_opt <- gamLoess_model$bestTune$span
deg_opt <- gamLoess_model$bestTune$degree

span_opt
```

Now that the optimal span value is identified as 0.4 (with the degree of 1), we will use these values to fit a final GAM model to predict on the test set.

```{r gam loess final}
# Or, if using caret's gamLoess method:
final_gamLoess_model_caret <- train(x = x, 
                                    y = y, 
                                    method = "gamLoess", 
                                    trControl = train_control, 
                                    tuneGrid = expand.grid(span = span_opt, degree = deg_opt))
```


# Simple Tree

Now we will fit a simple tree on this model. This is a nonparametric model due to the lack of assumption of any underlying information aside from the data itself. This model will tue on the cp value, which will allow us to know to what extent pruning should occur (i.e. how many splits the tree should have). Inference would be difficult for this model due to its free flowing, flexible structure. This model does perform variable selection by splitting on the variable deemed most siginificant first. Standardization of predictors does not need to occur to use this model appropriately.

```{r simple}
obes_simple <- rpart(type ~ .,
                     data = obes_train,
                     method = "class",
                     parms = list(split = "information"),
                     control = rpart.control(xval = 10,
                                             minbucket = 20,
                                             cp = 0.01)) # cv val, min val

obes_simple$cptable
```

With the simple tree model fit above, the tree with 10 splits has the minimum error. The value 1 standard error away (following the 1 standard error rule) is 9. We fit that final model with the inputed value below:

```{r final fit simple}
obes_simple_final <- prune(obes_simple,
                           cp = 0.01206140)
```


# Ensemble Tree
Next, we will fit an ensemble tree model using random forest bagging. This is a nonparametric model due to the lack of assumptions about the population distribution. This fit is tuning on the mtry value, which denotes the number of variables that can be taken into account at each cut (through a random sample). Similar to the simple tree model, general inference would be difficult due to the free form of the model fit. It also does perform variable selection due to each branch being representative of selecting a cutoff value for a certain predictor deemed important. Lastly, standardization of the predictors need not happen, since that will only adjust the cutoff value for each branch accordingly.

```{r ensemble}
obes_ensem <- train(type ~ .,
                    method = "rf",
                    data = obes_train,
                    tuneGrid = data.frame(mtry = 1:(ncol(obes_train)-1)),
                    trControl = trainControl(method = "oob", number = 2000))

obes_ensem$results |>
  round(4) |>
  kable()

ensem_opt <- obes_ensem$bestTune
ensem_opt
```

The best fit is the model that does a random sample of 8 predictors at each branch. Later, this model will be compared to the test set with all of the other best performing models.

```{r ensemble final}
obes_ensem_final <- train(type ~ .,
                    method = "rf",
                    data = obes_train,
                    tuneGrid = data.frame(mtry = ensem_opt),
                    trControl = trainControl(method = "none"))
```



# SVM

Lastly, we will fit a SVM model. The model being fit below is a parametric model due to its use of a linear kernel (defaulted). This model also is tuning the cost value, which is a value associated with the L value. This L value essentially determines how tolerant the model is to violations of the boundaries it is trying to create. This model is difficult to use for inference due to general takeaways from this model of the predictors and response being challenging in nature. In addition, this model does not perform variable selection. This means that the four variables contextually selected will be considered for this model. Lastly, standardization and scaling should occur in order to ensure that all points are placed in the hyperspace accordingly.


```{r svm, warning = FALSE}
tr <- trainControl(method = "repeatedcv",
                   number = 5, repeats = 10)

tune_grid <- expand.grid(cost = exp(seq(-5,3,len=30)))

sv_caret <- train(type ~ Age + family_history_with_overweight +
                    SMOKE + CAEC,
                  data = obes_train_stand,
                  method = "svmLinear2",
                  tuneGrid = tune_grid,
                  trControl = tr)

sv_caret$bestTune$cost
```

The best tune for the cost for this model is around 0.2432. This value will be used to fit the model to the entire training set, and that model will be compared to the other 5 models to choose an overall best performing model.


```{r svm final, eval = TRUE}
library(e1071)
sv_obes_final <- svm(type ~ Age + family_history_with_overweight +
                     SMOKE + CAEC,
                     data = obes_train_stand,
                     type = "C-classification",
                     kernel = "linear",
                     cost = sv_caret$bestTune$cost)
```


Now that all of the models have been fit and final models have been selected, we will compare each of these models to one another to determine the overall best fit.

# Final Model Comparison

Finally, we will compare the accuracy values for all of the chosen final models. This will be between the predicted weight class of each model's test set data and the true test set response. These are calculated and reported below:

```{r final comparions}
#kNN predict and accuracy computation
knn_resp <- as.factor(predict(knn_obes_final, obes_test_stand))
knn_mat <- caret::confusionMatrix(knn_resp, obes_test_stand$type)
knn <- as.numeric(knn_mat$overall[1])

#glmnet accuracy (predictions already computed)
logistic <- logLoss(glm_preds, as.numeric(obes_test$type))
glmnet_mat <- caret::confusionMatrix(glm_preds, as.factor(as.numeric(obes_test$type)))
glmnet <- as.numeric(glmnet_mat$overall[1])

#glm predict and accuracy computation
# Predict on test data (assuming you have obes_test)
x_test <- select(obes_test, c("Age", "family_history_with_overweight", "SMOKE", "CAEC"))
predictions <- predict(final_gamLoess_model_caret, newdata = x_test)
gamlo <- as.factor(as.numeric(predictions))
gam_mat <- caret::confusionMatrix(gamlo, as.factor(as.numeric(obes_test$type)))
gam <- as.numeric(gam_mat$overall[1])

#simple tree model probabilities, predicted classes, and log loss computation
simple_probs <- predict(obes_simple_final, obes_test)
simple_preds <- as.factor(apply(simple_probs, 1, which.max))
simple_mat <- caret::confusionMatrix(simple_preds, as.factor(as.numeric(obes_test$type)))
simple <- as.numeric(simple_mat$overall[1])

#ensemble tree model class label predictions and log loss computation
ensem_probs <- predict(obes_ensem_final, obes_test)
ensem_preds <- as.factor(as.numeric(ensem_probs))
ensemble_mat <- caret::confusionMatrix(ensem_preds,
                                       as.factor(as.numeric(obes_test$type)))
ensemble <- as.numeric(ensemble_mat$overall[1])

#svm
class_svm_preds <- predict(sv_obes_final, obes_test_stand)
svm_preds <- as.factor(as.numeric(class_svm_preds))
svm_mat <- caret::confusionMatrix(svm_preds,
                                  as.factor(as.numeric(obes_test_stand$type)))
svm <- as.numeric(svm_mat$overall[1])

c("knn" = knn, "glmnet" = glmnet, "gam" = gam,
  "s_tree" = simple, "e_tree" = ensemble, "svm" = svm)
```

The model with the highest accuracy is the ensemble tree with a random sample of 8 predictors taken into account at each cut. Because of this, we deem this model to be the overall best performing model for predicting weight class using the information in this data set. Finally, we will fit this model to the entire data set to have our overall best model.

# Best Model Fit

As stated above, we will fit the best performing model to the entire data set. In addition, we will investigate a plot showing variable importance for this model as well.

```{r best model}
obes_final_model <- train(type ~ .,
                    method = "rf",
                    data = obes,
                    tuneGrid = data.frame(mtry = ensem_opt),
                    trControl = trainControl(method = "none"))

plot(varImp(obes_final_model))
```

The table above shows the importance of each variable for predicting the response variable relative to the final ensemble tree model fit. Looking at the top of the plot, we selected 3 of the 4 deemed most important variables to use for analysis when variable selection did not occur. That is an indicator that the variables selected contextually were also selected with software as well.

Age, family history with overweight, gender, and whether one sometimes ate between meals were the variables that were most important in creating the final model that we have. Returning back to context, these entries are logical, due to encompassing physiology, genetics, and habits all into the final model.

To conclude, there are many different aspects of one's life that make weight issues more prevalent (or a heightened risk for them). Though some were deemed important in the final model over others, all of these aspects most likely play a role in the bigger picture of health and wellness. With that in mind, this model helps to paint a potential picture of what an oversimplified version of these relationships could look like.


Sources: 

Palechor, Fabio Mendoza and Alexis De la Hoz Manotas. “Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico.” Data in Brief 25 (2019): n. pag.
